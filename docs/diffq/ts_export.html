<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>diffq.ts_export API documentation</title>
<meta name="description" content="TorchScript export support.
We have to do a lot of black magic for TorchScript to be happy
because we cannot dynamically allocate new weights when â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>diffq.ts_export</code></h1>
</header>
<section id="section-intro">
<p>TorchScript export support.
We have to do a lot of black magic for TorchScript to be happy
because we cannot dynamically allocate new weights when loading the model.</p>
<p>Here is how it works:
- we generate code in a temporary python file for the given model that explicitely
override all the weights on the first forward from their packed version.
This is because TorchScript does not let us iterate over parameters in a generic manner.
- we zero out all the original weights. We cannot simply remove those weights
because TorchScript won't let us recreate them.
- A TorchScript file is just a zip file, but stored without compression.
In order to remove the cost of storing the zeroed out weights, we unzip the file,
and zip it again with compression.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
&#34;&#34;&#34;TorchScript export support.
We have to do a lot of black magic for TorchScript to be happy
because we cannot dynamically allocate new weights when loading the model.

Here is how it works:
- we generate code in a temporary python file for the given model that explicitely
    override all the weights on the first forward from their packed version.
    This is because TorchScript does not let us iterate over parameters in a generic manner.
- we zero out all the original weights. We cannot simply remove those weights
    because TorchScript won&#39;t let us recreate them.
- A TorchScript file is just a zip file, but stored without compression.
    In order to remove the cost of storing the zeroed out weights, we unzip the file,
    and zip it again with compression.
&#34;&#34;&#34;
import importlib
import os
from pathlib import Path
import random
import sys
import typing as tp
import tempfile
import zipfile

import torch
from torch import jit

from .diffq import DiffQuantizer
from .uniform import uniform_unquantize
from .torch_pack import unpack

_DiffQPacked = tp.Tuple[
    tp.List[tp.Optional[torch.Tensor]], tp.Tuple[float, float],
    torch.Tensor, tp.List[int]]

# This is the template for the generated class.
TEMPLATE = &#39;&#39;&#39;
import typing as tp
import torch
from torch import jit

from diffq.ts_export import _unpack_param, _DiffQPacked

from {module} import {klass}


class DiffQTSModel(torch.nn.Module):
    def __init__(self, model: {klass}, group_size: int, min_bits: int,
                 packed: tp.List[_DiffQPacked]):
        super().__init__()
        self.group_size = group_size
        self.min_bits = min_bits
        self.model = model
        self._unpacked = False
        self._packed = packed

    @jit.export
    def unpack(self):
        &#34;&#34;&#34;
        Unpack the weights, automatically called on the first forward,
        or explicitely.&#34;&#34;&#34;
        if self._unpacked:
            return
{unpack_assigns}
        self._unpacked = True

    def forward(self, x: torch.Tensor):
        self.unpack()
        return self.model.forward(x)
&#39;&#39;&#39;

# those are the assignments for each quantized weight.
UNPACK_ASSIGN = (&#39; &#39; * 8) + (&#39;self.model{full_name}.data[:] = &#39;
                             &#39;_unpack_param(self._packed[{index}], &#39;
                             &#39;group_size=self.group_size, min_bits=self.min_bits)&#39;)
UNPACK_ASSIGN_SAME = (&#39; &#39; * 8) + &#39;self.model{full_name} = self.model{other_name}&#39;


def export(quantizer: DiffQuantizer, path: tp.Union[str, Path]):
    &#34;&#34;&#34;Export the given quantized model to the given path.
    We must save the quantized model ourselves, as we need to recompress
    the zip archive afterwards.

    ..Warning:: This will completely destroy the model and the quantizer, so you probably
        want to call this only once at the end of training.
    &#34;&#34;&#34;
    packed: tp.List[_DiffQPacked] = []
    uniq_name = &#39;&#39;.join([random.choice(&#34;abcdefghijklmnopqrstuvwxyz&#34;) for _ in range(12)])
    with tempfile.TemporaryDirectory() as tmpdir:
        sys.path.insert(0, tmpdir)
        try:
            code = _codegen(quantizer)
            with open(Path(tmpdir) / f&#39;{uniq_name}.py&#39;, &#39;w&#39;) as f:
                f.write(code)
            module = importlib.import_module(uniq_name)
            ts_klass = module.DiffQTSModel
            state = quantizer.get_quantized_state(packed=True, torch_pack=True)
            quantized = state[&#34;quantized&#34;]
            for qparam in quantizer._qparams:
                if qparam.other is None:
                    levels, scales, bits = quantized.pop(0)
                    size = qparam.param.size()
                    packed.append((levels, scales, bits, list(size)))
                    qparam.param.data.zero_()
            quantizer.detach()
            ts_premodel = ts_klass(quantizer.model, quantizer.group_size,
                                   quantizer.min_bits, packed)
            ts_model = jit.script(ts_premodel)
            if path is not None:
                jit.save(ts_model, path)
                recompress(path)
        finally:
            sys.path.pop(0)

    return ts_model


def _unpack_param(packed: _DiffQPacked, group_size: int, min_bits: int) -&gt; torch.Tensor:
    &#34;&#34;&#34;Function called from TorchScript on the first forward to decode the
    packed weights to FP32.
    &#34;&#34;&#34;
    packed_all_levels, scales, packed_bits, shape = packed
    numel = 1
    for dim in shape:
        numel *= dim
    bits = unpack(packed_bits, numel // group_size) + min_bits
    levels = torch.empty(bits.numel(), group_size, dtype=torch.short)
    for idx, packed_levels in enumerate(packed_all_levels):
        bit = idx + 1
        if packed_levels is not None:
            sub_levels = levels[bits == bit]
            levels[bits == bit] = unpack(packed_levels, sub_levels.numel()).view_as(sub_levels)
    bits = bits[:, None]
    unquant = uniform_unquantize(levels, scales, bits)
    if len(shape) == 4:
        return unquant.view(shape[0], shape[1], shape[2], shape[3])
    elif len(shape) == 3:
        return unquant.view(shape[0], shape[1], shape[2])
    elif len(shape) == 2:
        return unquant.view(shape[0], shape[1])
    elif len(shape) == 1:
        return unquant.view(shape[0])
    else:
        raise RuntimeError(&#34;Invalid numbr of dim&#34;)


def recompress(path: tp.Union[str, Path]):
    &#34;&#34;&#34;After having saved the torchscript file, this will recompress it
    to make sure all the zeroed out parameters don&#39;t actually take any space.
    &#34;&#34;&#34;
    with tempfile.TemporaryDirectory() as tmpdir:
        with zipfile.ZipFile(path) as zipin:
            zipin.extractall(tmpdir)
        with zipfile.ZipFile(path, &#34;w&#34;, compression=zipfile.ZIP_DEFLATED,
                             compresslevel=1) as zipout:
            for root, folders, files in os.walk(tmpdir):
                for file in files:
                    fp = Path(root) / file
                    name = fp.relative_to(tmpdir)
                    zipout.write(fp, name)


def _get_full_name_access(full_name):
    # When generating code, we need to handle attributes vs. indexing.
    parts = []
    for part in full_name.split(&#34;.&#34;):
        try:
            index = int(part)
        except ValueError:
            parts.append(&#34;.&#34; + part)
        else:
            parts.append(f&#34;[{index}]&#34;)
    return &#34;&#34;.join(parts)


def _codegen(quantizer: DiffQuantizer):
    # Generates the code for the given quantizer
    module = quantizer.model.__class__.__module__
    klass = quantizer.model.__class__.__name__
    model = quantizer.model

    assert not quantizer.float16
    names = {}
    for mod_name, mod in model.named_modules():
        names[mod] = mod_name
    unpack_assigns = []

    index = 0
    for qparam in quantizer._qparams:
        mod_name = names[qparam.module]
        if mod_name == &#39;&#39;:
            full_name = qparam.name
        else:
            full_name = mod_name + &#39;.&#39; + qparam.name
        full_name = _get_full_name_access(full_name)
        if qparam.other is None:
            unpack_assigns.append(UNPACK_ASSIGN.format(full_name=full_name, index=index))
            index += 1
        else:
            other_name = names[(qparam.other.module, qparam.other.name)]
            other_name = _get_full_name_access(other_name)
            unpack_assigns.append(
                UNPACK_ASSIGN_SAME.format(full_name=full_name, other_name=other_name))

    return TEMPLATE.format(
        module=module,
        klass=klass,
        unpack_assigns=&#39;\n&#39;.join(unpack_assigns))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="diffq.ts_export.export"><code class="name flex">
<span>def <span class="ident">export</span></span>(<span>quantizer:Â <a title="diffq.diffq.DiffQuantizer" href="diffq.html#diffq.diffq.DiffQuantizer">DiffQuantizer</a>, path:Â Union[str,Â pathlib.Path])</span>
</code></dt>
<dd>
<div class="desc"><p>Export the given quantized model to the given path.
We must save the quantized model ourselves, as we need to recompress
the zip archive afterwards.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;This will completely destroy the model and the quantizer, so you probably</p>
<p>want to call this only once at the end of training.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export(quantizer: DiffQuantizer, path: tp.Union[str, Path]):
    &#34;&#34;&#34;Export the given quantized model to the given path.
    We must save the quantized model ourselves, as we need to recompress
    the zip archive afterwards.

    ..Warning:: This will completely destroy the model and the quantizer, so you probably
        want to call this only once at the end of training.
    &#34;&#34;&#34;
    packed: tp.List[_DiffQPacked] = []
    uniq_name = &#39;&#39;.join([random.choice(&#34;abcdefghijklmnopqrstuvwxyz&#34;) for _ in range(12)])
    with tempfile.TemporaryDirectory() as tmpdir:
        sys.path.insert(0, tmpdir)
        try:
            code = _codegen(quantizer)
            with open(Path(tmpdir) / f&#39;{uniq_name}.py&#39;, &#39;w&#39;) as f:
                f.write(code)
            module = importlib.import_module(uniq_name)
            ts_klass = module.DiffQTSModel
            state = quantizer.get_quantized_state(packed=True, torch_pack=True)
            quantized = state[&#34;quantized&#34;]
            for qparam in quantizer._qparams:
                if qparam.other is None:
                    levels, scales, bits = quantized.pop(0)
                    size = qparam.param.size()
                    packed.append((levels, scales, bits, list(size)))
                    qparam.param.data.zero_()
            quantizer.detach()
            ts_premodel = ts_klass(quantizer.model, quantizer.group_size,
                                   quantizer.min_bits, packed)
            ts_model = jit.script(ts_premodel)
            if path is not None:
                jit.save(ts_model, path)
                recompress(path)
        finally:
            sys.path.pop(0)

    return ts_model</code></pre>
</details>
</dd>
<dt id="diffq.ts_export.recompress"><code class="name flex">
<span>def <span class="ident">recompress</span></span>(<span>path:Â Union[str,Â pathlib.Path])</span>
</code></dt>
<dd>
<div class="desc"><p>After having saved the torchscript file, this will recompress it
to make sure all the zeroed out parameters don't actually take any space.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recompress(path: tp.Union[str, Path]):
    &#34;&#34;&#34;After having saved the torchscript file, this will recompress it
    to make sure all the zeroed out parameters don&#39;t actually take any space.
    &#34;&#34;&#34;
    with tempfile.TemporaryDirectory() as tmpdir:
        with zipfile.ZipFile(path) as zipin:
            zipin.extractall(tmpdir)
        with zipfile.ZipFile(path, &#34;w&#34;, compression=zipfile.ZIP_DEFLATED,
                             compresslevel=1) as zipout:
            for root, folders, files in os.walk(tmpdir):
                for file in files:
                    fp = Path(root) / file
                    name = fp.relative_to(tmpdir)
                    zipout.write(fp, name)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="diffq" href="index.html">diffq</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="diffq.ts_export.export" href="#diffq.ts_export.export">export</a></code></li>
<li><code><a title="diffq.ts_export.recompress" href="#diffq.ts_export.recompress">recompress</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>