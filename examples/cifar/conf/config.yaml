defaults:
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog
  - preset: default

db:
  name: 'cifar10'  # available options are cifar10 and cifar100 at the moment
  root: './data'  # path to save the data

# Logging and printing, and does not impact training
num_prints: 5
device: cuda
num_workers: 5
verbose: 0
show: 0   # just show the model and its size and exit

# Checkpointing, by default automatically load last checkpoint
checkpoint: True
continue_from: '' # Only pass the name of the exp, like `exp_dset=wham`
                  # this arg is ignored for the naming of the exp!
continue_best: false
restart: False # Ignore existing checkpoints
checkpoint_file: checkpoint.th
history_file: history.json

# Other stuff
seed: 2036

# Optimization related
epochs: 200
optim: sgd
lr: 0.1
momentum: 0.9
w_decay: 5e-4
batch_size: 128
mixed: True  # if true, uses mixed precision training
beta2: 0.999
max_norm: 5
nesterov: True
alpha: 0.9

# learning rate scheduling
lr_sched: step # can be either step or plateau
step:
  step_size: 60
  gamma: 0.2
multistep:
  gamma: 0.1
plateau:
  factor: 0.5
  patience: 5
cosine:
  T_max: 10
  min_lr: 0.0001

# Models
model: resnet # supported options are - resnet | mobilenet | w_resnet

quant:
  min_size: 0.01  # minimum param size in MB to be quantized
  bits: 0  # number of bits used for uniform quantization
  penalty: 0  # model weight penalty for DiffQ
  group_size: 8  # group size for DiffQ
  min_bits: 2  # minimal number of bits for DiffQ
  init_bits: 8  # initial number of bits for DiffQ
  max_bits: 15  # max number of bits for DiffQ
  exclude: []  # exclude patterns, e.g. bias
  qat: False  # quantization aware training to be used with uniform qunatization
  lr: 1e-3  # learning rate for the bits parameters
  adam: True  # use a separate optimizer for the bits parameters
  lsq: False  # use LSQ


rendezvous_file: ./rendezvous
ddp: 0  # if set to true, uses all available GPUs.
rank:
world_size: 1

# Hydra config
hydra:
  run:
    dir: ./outputs/exp_${hydra.job.override_dirname}
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        # Remove params that would not,t impact the training itself
        # Remove all slurm and submit params.
        # This is ugly I know but probably faster than recoding Hydra
        exclude_keys: [
          'db.root',
          'hydra.job_logging.handles.file.filename',
          'num_prints', 'continue_from',
          'device', 'num_workers', 'print_freq', 'restart', 'verbose',
          'rendezvous_file', 'ddp', 'rank', 'world_size']
  # No clue what is the difference between hydra and job logging...
  job_logging:
    formatters:
      colorlog:
        datefmt: "%m-%d %H:%M:%S"
    handlers:
      file:
        class: logging.FileHandler
        mode: w
        formatter: colorlog
        filename: local.log
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr

  hydra_logging:
    handlers:
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr
